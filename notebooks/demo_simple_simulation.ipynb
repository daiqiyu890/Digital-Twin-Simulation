{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Twin Simulation - Simple Demo\n",
    "\n",
    "This notebook presents simple examples of how to leverage the persona to simulate survey responses of new questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai==1.78.1 pandas==2.2.2 datasets==2.18.0\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List\n",
    "import openai\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Enter your API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = input(\"API Key: \").strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "NUM_PERSONAS = 30  # Number of personas to load (max ~2058 available)\n",
    "\n",
    "# Check and install datasets library if needed\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "except ImportError:\n",
    "    print(\"Installing datasets library...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\"])\n",
    "    from datasets import load_dataset\n",
    "    print(\"✅ datasets library installed successfully\")\n",
    "\n",
    "# Download and load persona summaries directly from Hugging Face\n",
    "def load_personas(num_personas=30):\n",
    "    \"\"\"Download and load persona summaries directly from Hugging Face dataset.\"\"\"\n",
    "    \n",
    "    print(f\"Loading {num_personas} persona summaries from Hugging Face...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the dataset directly from Hugging Face\n",
    "        dataset = load_dataset(\"LLM-Digital-Twin/Twin-2K-500\", 'full_persona', split='data')\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading dataset: {type(e).__name__}: {str(e)}\")\n",
    "        print(\"\\nTrying to clear cache and reload...\")\n",
    "        \n",
    "        # Clear the cache for this specific dataset\n",
    "        import shutil\n",
    "        from pathlib import Path\n",
    "        \n",
    "        # Get the default cache directory\n",
    "        cache_dir = Path.home() / \".cache\" / \"huggingface\" / \"datasets\" / \"LLM-Digital-Twin___parquet\"\n",
    "        \n",
    "        if cache_dir.exists():\n",
    "            print(f\"Clearing cache directory: {cache_dir}\")\n",
    "            shutil.rmtree(cache_dir)\n",
    "        \n",
    "        # Try loading again with download_mode='force_redownload'\n",
    "        try:\n",
    "            dataset = load_dataset(\"LLM-Digital-Twin/Twin-2K-500\", 'full_persona', split='data', download_mode='force_redownload')\n",
    "            print(\"✅ Dataset loaded successfully after clearing cache\")\n",
    "        except Exception as e2:\n",
    "            print(f\"❌ Still unable to load dataset: {type(e2).__name__}: {str(e2)}\")\n",
    "            print(\"\\nAlternative: You can manually download the dataset from:\")\n",
    "            print(\"https://huggingface.co/datasets/LLM-Digital-Twin/Twin-2K-500\")\n",
    "            raise e2\n",
    "    \n",
    "    # Extract personas\n",
    "    personas = {}\n",
    "    pids = dataset[\"pid\"]\n",
    "    persona_summaries = dataset[\"persona_summary\"]\n",
    "    \n",
    "    # Load requested number of personas\n",
    "    for i in range(min(num_personas, len(pids))):\n",
    "        pid = pids[i]\n",
    "        summary = persona_summaries[i]\n",
    "        \n",
    "        if summary is not None:\n",
    "            personas[f\"pid_{pid}\"] = summary\n",
    "    \n",
    "    return personas\n",
    "\n",
    "# Load personas\n",
    "personas = load_personas(NUM_PERSONAS)\n",
    "\n",
    "print(f\"✅ Loaded {len(personas)} personas\")\n",
    "\n",
    "# Show sample of first persona\n",
    "if personas:\n",
    "    first_persona = list(personas.values())[0]\n",
    "    print(f\"\\nSample persona (first 500 chars):\")\n",
    "    print(\"=\"*50)\n",
    "    print(first_persona[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Questions and Simulate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_responses(personas, template):\n",
    "    rows = []\n",
    "    for pid, persona in personas.items():\n",
    "        user_msg = template.format(persona=persona)\n",
    "        try:\n",
    "            resp = openai.chat.completions.create(\n",
    "                model=\"gpt-4.1-mini-2025-04-14\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "                    {\"role\": \"user\",   \"content\": user_msg}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=5  # enough for a single number\n",
    "            )\n",
    "            answer = resp.choices[0].message.content.strip()\n",
    "            print(f\"✅ {pid}: {answer}\")\n",
    "        except Exception as e:\n",
    "            answer = f\"Error: {e}\"\n",
    "            print(f\"❌ {pid}: {answer}\")\n",
    "        rows.append({\"persona_id\": pid, \"answer\": answer})\n",
    "        time.sleep(0.5)  # gentle rate‑limit\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"You, AI, are an expert in predicting human responses to questions. You are given a persona profile and a question, and also a format instructions that specifies the type of answer you need to provide. You need to answer the question as the persona would answer it, based on the persona profile and the format instructions.\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"\n",
    "{persona}\n",
    "\n",
    "QUESTION: Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Please complete the statements below.\n",
    "\n",
    "It is ___ that Linda is a teacher in an elementary school.\n",
    "\n",
    "Options:\n",
    "  1 = Extremely improbable\n",
    "  2 = Very improbable\n",
    "  3 = Somewhat probable\n",
    "  4 = Moderately probable\n",
    "  5 = Very probable\n",
    "  6 = Extremely probable\n",
    "\n",
    "FORMAT INSTRUCTIONS: Only return the number, no other text.\n",
    "\"\"\"\n",
    "\n",
    "df = simulate_responses(personas, USER_PROMPT_TEMPLATE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digital-twin-simulation-o7RtJ_Ea-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
