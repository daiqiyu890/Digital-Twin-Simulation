{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Twin Simulation - Full Pipeline Demo with Evaluation\n",
    "\n",
    "This notebook demonstrates the complete digital twin simulation pipeline, including:\n",
    "1. Downloading the dataset\n",
    "2. Processing personas and questions\n",
    "3. Running LLM simulations\n",
    "4. Converting results to CSV format\n",
    "5. Running accuracy evaluations\n",
    "6. Performing pricing analysis\n",
    "\n",
    "**Important**: \n",
    "- This notebook should be run from the `notebooks/` directory\n",
    "- Make sure you have activated the poetry environment: `poetry shell`\n",
    "- Ensure you have set your OPENAI_API_KEY in the `.env` file at the project root\n",
    "\n",
    "**Note**: This notebook runs the full pipeline with a small subset of personas for demonstration. For production use, follow the shell scripts in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Direct path setup - adjust this path if your project is in a different location\n",
    "PROJECT_ROOT_PATH = \"/Users/qiyudai/Documents/Github/Digital-Twin-Simulation\"\n",
    "\n",
    "# Set up project root\n",
    "project_root = Path(PROJECT_ROOT_PATH)\n",
    "\n",
    "# Verify the project root exists and has expected directories\n",
    "if not project_root.exists():\n",
    "    raise RuntimeError(f\"Project root not found at: {project_root}\")\n",
    "\n",
    "if not (project_root / 'text_simulation').exists():\n",
    "    raise RuntimeError(f\"'text_simulation' directory not found in: {project_root}\")\n",
    "\n",
    "if not (project_root / 'evaluation').exists():\n",
    "    raise RuntimeError(f\"'evaluation' directory not found in: {project_root}\")\n",
    "\n",
    "# Add project root to Python path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Configuration\n",
    "MAX_PERSONAS = 2  # Limit for demo purposes\n",
    "\n",
    "print(f\"✅ Project root: {project_root}\")\n",
    "print(f\"Current directory: {Path.cwd()}\")\n",
    "print(f\"Python path configured: {sys.path[0]}\")\n",
    "\n",
    "# Set notebook directory\n",
    "notebook_dir = project_root / 'notebooks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "print(\"=\" * 60)\n",
    "print(\"Digital Twin Simulation - Full Pipeline Demo\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(project_root / '.env')\n",
    "\n",
    "# Check OpenAI API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"⚠️  Please set your OPENAI_API_KEY in the .env file\")\n",
    "else:\n",
    "    print(\"✅ OpenAI API key loaded successfully\")\n",
    "\n",
    "print(f\"\\nConfigured to process {MAX_PERSONAS} personas for this demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Dataset\n",
    "\n",
    "First, we'll download the Twin-2K-500 dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Step 1: Download Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_dir = project_root / \"data\"\n",
    "if (data_dir / \"mega_persona_json\" / \"mega_persona\").exists():\n",
    "    print(\"✅ Dataset already downloaded\")\n",
    "else:\n",
    "    print(\"Downloading dataset...\")\n",
    "    # Save current directory\n",
    "    original_cwd = Path.cwd()\n",
    "    \n",
    "    try:\n",
    "        # Change to project root for download\n",
    "        os.chdir(project_root)\n",
    "        \n",
    "        # Import and run the download function directly\n",
    "        import download_dataset\n",
    "        download_dataset.main()\n",
    "        print(\"✅ Dataset downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error downloading dataset: {e}\")\n",
    "    finally:\n",
    "        # Restore original directory\n",
    "        os.chdir(original_cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Update Configuration\n",
    "\n",
    "Update the configuration file to limit processing to our demo size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Step 2: Update Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "config_path = project_root / \"text_simulation\" / \"configs\" / \"openai_config.yaml\"\n",
    "\n",
    "try:\n",
    "    # Read current config\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Update max_personas\n",
    "    config['max_personas'] = MAX_PERSONAS\n",
    "    \n",
    "    # Write back\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"✅ Updated config to process {MAX_PERSONAS} personas\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error updating config: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert Personas to Text Format\n",
    "\n",
    "Convert the JSON persona data into text format suitable for LLM processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Step 3: Convert Personas to Text\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import the conversion function\n",
    "from text_simulation.convert_persona_to_text import convert_persona_to_text\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up paths\n",
    "persona_json_dir = project_root / \"data\" / \"mega_persona_json\" / \"mega_persona\"\n",
    "output_text_dir = project_root / \"text_simulation\" / \"text_personas\"\n",
    "\n",
    "# Create output directory\n",
    "output_text_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Get persona files and limit to MAX_PERSONAS for demo\n",
    "    json_files = [f for f in os.listdir(persona_json_dir) \n",
    "                  if f.endswith('.json') and f.startswith('pid_')]\n",
    "    \n",
    "    # Limit files for demo\n",
    "    files_to_process = json_files[:MAX_PERSONAS]\n",
    "    \n",
    "    print(f\"Converting {len(files_to_process)} personas (limited for demo)...\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for json_file in tqdm(files_to_process, desc=\"Converting personas\"):\n",
    "        input_path = persona_json_dir / json_file\n",
    "        output_path = output_text_dir / json_file.replace('.json', '.txt')\n",
    "        \n",
    "        if convert_persona_to_text(str(input_path), str(output_path), \"full\"):\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "            print(f\"Failed to convert {json_file}\")\n",
    "    \n",
    "    print(f\"\\n✅ Conversion complete. Successful: {successful}, Failed: {failed}\")\n",
    "    \n",
    "    # Check output directory\n",
    "    persona_files = list(output_text_dir.glob(\"*.txt\"))\n",
    "    print(f\"   Created {len(persona_files)} persona text files\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error converting personas: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert Questions to Text Format\n",
    "\n",
    "Process the survey questions into a format suitable for the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Step 4: Convert Questions to Text\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use subprocess to run the script with proper Python path\n",
    "import subprocess\n",
    "\n",
    "# Run the script with PYTHONPATH set to include text_simulation directory\n",
    "env = os.environ.copy()\n",
    "env['PYTHONPATH'] = str(project_root / 'text_simulation') + os.pathsep + env.get('PYTHONPATH', '')\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, str(project_root / \"text_simulation\" / \"convert_question_json_to_text.py\")],\n",
    "    cwd=str(project_root),\n",
    "    env=env,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✅ Questions converted successfully\")\n",
    "    \n",
    "    # Check output\n",
    "    output_dir = project_root / \"text_simulation\" / \"text_questions\"\n",
    "    if output_dir.exists():\n",
    "        question_files = list(output_dir.glob(\"*.txt\"))\n",
    "        print(f\"   Created {len(question_files)} question text files\")\n",
    "else:\n",
    "    print(f\"❌ Error converting questions: {result.stderr}\")\n",
    "    # If it still fails, suggest manual fix\n",
    "    print(\"\\nNote: If this continues to fail, you can run manually:\")\n",
    "    print(f\"  cd {project_root}\")\n",
    "    print(\"  python text_simulation/convert_question_json_to_text.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Simulation Input\n",
    "\n",
    "Combine personas with questions to create the input files for simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Step 5: Create Simulation Input\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import the function\n",
    "from text_simulation.create_text_simulation_input import create_combined_prompts\n",
    "\n",
    "# Set up paths\n",
    "persona_text_dir = str(project_root / \"text_simulation\" / \"text_personas\")\n",
    "question_prompts_dir = str(project_root / \"text_simulation\" / \"text_questions\")\n",
    "output_combined_prompts_dir = str(project_root / \"text_simulation\" / \"text_simulation_input\")\n",
    "\n",
    "try:\n",
    "    create_combined_prompts(\n",
    "        persona_text_dir=persona_text_dir,\n",
    "        question_prompts_dir=question_prompts_dir,\n",
    "        output_combined_prompts_dir=output_combined_prompts_dir\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Simulation input created successfully\")\n",
    "    \n",
    "    # Check how many input files were created\n",
    "    input_dir = Path(output_combined_prompts_dir)\n",
    "    if input_dir.exists():\n",
    "        prompt_files = list(input_dir.glob(\"*_prompt.txt\"))\n",
    "        print(f\"   Created {len(prompt_files)} prompt files\")\n",
    "        \n",
    "        # Limit to MAX_PERSONAS for demo\n",
    "        if len(prompt_files) > MAX_PERSONAS:\n",
    "            print(f\"   (Will process only first {MAX_PERSONAS} for this demo)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating simulation input: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run LLM Simulations\n",
    "\n",
    "Now we'll run the actual LLM simulations. This will use the OpenAI API to generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display current configuration\n",
    "config_path = project_root / \"text_simulation\" / \"configs\" / \"openai_config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Current simulation configuration:\")\n",
    "print(f\"  Model: {config['model_name']}\")\n",
    "print(f\"  Temperature: {config['temperature']}\")\n",
    "print(f\"  Max personas: {config['max_personas']}\")\n",
    "print(f\"  Workers: {config['num_workers']}\")\n",
    "print(f\"  Force regenerate: {config['force_regenerate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Step 6: Run LLM Simulations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nRunning LLM simulations...\")\n",
    "print(\"This may take a few minutes depending on the number of personas and API rate limits.\\n\")\n",
    "\n",
    "# Use subprocess to run the simulation with proper Python path\n",
    "import subprocess\n",
    "\n",
    "# Set up environment with proper PYTHONPATH\n",
    "env = os.environ.copy()\n",
    "env['PYTHONPATH'] = str(project_root) + os.pathsep + str(project_root / 'text_simulation') + os.pathsep + env.get('PYTHONPATH', '')\n",
    "\n",
    "# Run the simulation script\n",
    "process = subprocess.Popen(\n",
    "    [\n",
    "        sys.executable, \n",
    "        str(project_root / \"text_simulation\" / \"run_LLM_simulations.py\"),\n",
    "        \"--config\", str(project_root / \"text_simulation\" / \"configs\" / \"openai_config.yaml\"),\n",
    "        \"--max_personas\", str(MAX_PERSONAS)\n",
    "    ],\n",
    "    cwd=str(project_root),\n",
    "    env=env,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Stream output\n",
    "try:\n",
    "    for line in process.stdout:\n",
    "        print(line.rstrip())\n",
    "    \n",
    "    process.wait()\n",
    "    \n",
    "    if process.returncode == 0:\n",
    "        print(\"\\n✅ Simulations completed successfully\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Error running simulations (exit code: {process.returncode})\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⚠️  Simulation interrupted by user\")\n",
    "    process.terminate()\n",
    "    process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Examine Simulation Results\n",
    "\n",
    "Let's look at some of the generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Step 7: Examine Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "output_dir = project_root / \"text_simulation\" / \"text_simulation_output\"\n",
    "\n",
    "if output_dir.exists():\n",
    "    persona_dirs = [d for d in output_dir.iterdir() if d.is_dir() and d.name.startswith(\"pid_\")]\n",
    "    print(f\"Found {len(persona_dirs)} persona output directories\\n\")\n",
    "    \n",
    "    # Show a sample response\n",
    "    if persona_dirs:\n",
    "        sample_dir = persona_dirs[0]\n",
    "        response_files = list(sample_dir.glob(\"*_response.json\"))\n",
    "        \n",
    "        if response_files:\n",
    "            with open(response_files[0], 'r') as f:\n",
    "                response = json.load(f)\n",
    "            \n",
    "            print(f\"Sample response from {sample_dir.name}:\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"Question ID: {response.get('question_id', 'N/A')}\")\n",
    "            print(f\"\\nPrompt (first 200 chars):\")\n",
    "            print(response.get('prompt_text', '')[:200] + \"...\")\n",
    "            print(f\"\\nResponse (first 500 chars):\")\n",
    "            response_text = response.get('response_text', 'No response')\n",
    "            if len(response_text) > 500:\n",
    "                print(response_text[:500] + \"...\")\n",
    "            else:\n",
    "                print(response_text)\n",
    "            print(\"=\" * 50)\n",
    "else:\n",
    "    print(\"No output directory found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Convert JSON to CSV for Evaluation\n",
    "\n",
    "Convert the JSON answer blocks to CSV format for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Step 8: Convert JSON to CSV for Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create evaluation config for json2csv\n",
    "eval_config = {\n",
    "    \"trial_dir\": \"text_simulation/text_simulation_output/\",\n",
    "    \"model_name\": \"gpt-4.1-mini\",\n",
    "    \"max_personas\": MAX_PERSONAS,\n",
    "    \"waves\": {\n",
    "        \"wave1_3\": {\n",
    "            \"input_pattern\": \"data/mega_persona_json/answer_blocks/pid_{pid}_wave4_Q_wave1_3_A.json\",\n",
    "            \"output_csv\": \"${trial_dir}/csv_comparison/responses_wave1_3.csv\",\n",
    "            \"output_csv_formatted\": \"${trial_dir}/csv_comparison/csv_formatted/responses_wave1_3_formatted.csv\",\n",
    "            \"output_csv_labeled\": \"${trial_dir}/csv_comparison/csv_formatted_label/responses_wave1_3_label_formatted.csv\"\n",
    "        },\n",
    "        \"wave4\": {\n",
    "            \"input_pattern\": \"data/mega_persona_json/answer_blocks/pid_{pid}_wave4_Q_wave4_A.json\",\n",
    "            \"output_csv\": \"${trial_dir}/csv_comparison/responses_wave4.csv\",\n",
    "            \"output_csv_formatted\": \"${trial_dir}/csv_comparison/csv_formatted/responses_wave4_formatted.csv\",\n",
    "            \"output_csv_labeled\": \"${trial_dir}/csv_comparison/csv_formatted_label/responses_wave4_label_formatted.csv\"\n",
    "        },\n",
    "        \"llm_imputed\": {\n",
    "            \"input_pattern\": \"${trial_dir}/answer_blocks_llm_imputed/pid_{pid}_wave4_Q_wave4_A.json\",\n",
    "            \"output_csv\": \"${trial_dir}/csv_comparison/responses_llm_imputed.csv\",\n",
    "            \"output_csv_formatted\": \"${trial_dir}/csv_comparison/csv_formatted/responses_llm_imputed_formatted.csv\",\n",
    "            \"output_csv_labeled\": \"${trial_dir}/csv_comparison/csv_formatted_label/responses_llm_imputed_label_formatted.csv\"\n",
    "        }\n",
    "    },\n",
    "    \"benchmark_csv\": \"data/wave_csv/wave_4_numbers_anonymized.csv\",\n",
    "    \"column_mapping\": \"evaluation/column_mapping.csv\",\n",
    "    \"save_question_mapping\": True,\n",
    "    \"question_mapping_output\": \"${trial_dir}/csv_comparison/question_mapping.csv\",\n",
    "    \"generate_randdollar_breakdown\": True,\n",
    "    \"randdollar_output\": \"${trial_dir}/csv_comparison/randdollar_breakdown.csv\"\n",
    "}\n",
    "\n",
    "# Write temporary config file\n",
    "temp_eval_config = project_root / \"temp_eval_config.yaml\"\n",
    "with open(temp_eval_config, 'w') as f:\n",
    "    yaml.dump(eval_config, f)\n",
    "\n",
    "print(\"Converting JSON results to CSV format...\")\n",
    "\n",
    "# Run json2csv conversion\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"evaluation/json2csv.py\", \"--config\", str(temp_eval_config), \"--all\", \"--verbose\"],\n",
    "    cwd=str(project_root),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✅ JSON to CSV conversion completed successfully\")\n",
    "    \n",
    "    # Check what was created\n",
    "    csv_dir = project_root / \"text_simulation\" / \"text_simulation_output\" / \"csv_comparison\"\n",
    "    if csv_dir.exists():\n",
    "        csv_files = list((csv_dir / \"csv_formatted\").glob(\"*.csv\")) if (csv_dir / \"csv_formatted\").exists() else []\n",
    "        print(f\"   Generated {len(csv_files)} formatted CSV files\")\n",
    "        if csv_files:\n",
    "            print(\"   Files created:\")\n",
    "            for f in csv_files[:5]:  # Show first 5 files\n",
    "                print(f\"     - {f.name}\")\n",
    "else:\n",
    "    print(f\"⚠️  JSON to CSV conversion encountered issues\")\n",
    "    print(f\"   Error: {result.stderr[:500]}...\")  # Show first 500 chars of error\n",
    "    \n",
    "# Clean up temp config\n",
    "if temp_eval_config.exists():\n",
    "    temp_eval_config.unlink()\n",
    "\n",
    "print(f\"\\nOutput directory: {csv_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MAD Accuracy Evaluation\n",
    "\n",
    "Run Mean Absolute Difference (MAD) accuracy evaluation to compare simulated responses with ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Step 9: MAD Accuracy Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set up paths for MAD evaluation\n",
    "trial_dir = project_root / \"text_simulation\" / \"text_simulation_output\"\n",
    "csv_dir = trial_dir / \"csv_comparison\" / \"csv_formatted\"\n",
    "output_dir = trial_dir / \"accuracy_evaluation\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create MAD evaluation config\n",
    "mad_config = {\n",
    "    \"csv_dir\": str(csv_dir),\n",
    "    \"output_dir\": str(output_dir),\n",
    "    \"output_excel_filename\": \"mad_accuracy_summary.xlsx\",\n",
    "    \"output_plot_filename\": \"accuracy_dist.png\",\n",
    "    \"plot_title\": \"Digital Twin Simulation - GPT-4.1-mini\"\n",
    "}\n",
    "\n",
    "# Write temporary config file\n",
    "temp_mad_config = project_root / \"temp_mad_config.yaml\"\n",
    "with open(temp_mad_config, 'w') as f:\n",
    "    yaml.dump(mad_config, f)\n",
    "\n",
    "print(\"Computing MAD accuracy metrics...\")\n",
    "\n",
    "# Check if required CSV files exist\n",
    "required_files = [\"responses_wave1_3_formatted.csv\", \"responses_wave4_formatted.csv\", \"responses_llm_imputed_formatted.csv\"]\n",
    "missing_files = [f for f in required_files if not (csv_dir / f).exists()]\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"⚠️  Missing required CSV files: {missing_files}\")\n",
    "    print(\"   Skipping MAD evaluation...\")\n",
    "else:\n",
    "    # Run MAD evaluation\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, \"evaluation/mad_accuracy_evaluation.py\", \"--config\", str(temp_mad_config), \"--verbose\"],\n",
    "        cwd=str(project_root),\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ MAD evaluation completed successfully\")\n",
    "        print(f\"   Results saved to: {output_dir}\")\n",
    "        \n",
    "        # Check outputs\n",
    "        excel_file = output_dir / \"mad_accuracy_summary.xlsx\"\n",
    "        plot_file = output_dir / \"accuracy_dist.png\"\n",
    "        \n",
    "        if excel_file.exists():\n",
    "            print(f\"   ✅ Excel summary: {excel_file.name}\")\n",
    "        if plot_file.exists():\n",
    "            print(f\"   ✅ Accuracy plot: {plot_file.name}\")\n",
    "            \n",
    "            # Try to display the plot if in notebook environment\n",
    "            try:\n",
    "                from IPython.display import Image, display\n",
    "                display(Image(str(plot_file)))\n",
    "            except:\n",
    "                print(\"   (Plot saved but cannot display inline)\")\n",
    "    else:\n",
    "        print(f\"⚠️  MAD evaluation encountered issues\")\n",
    "        print(f\"   Error: {result.stderr[:500]}...\")\n",
    "\n",
    "# Clean up temp config\n",
    "if temp_mad_config.exists():\n",
    "    temp_mad_config.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Within-Between Subjects Analysis\n",
    "\n",
    "This analysis examines behavioral economics experiments across different waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Step 10: Within-Between Subjects Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import the analysis classes directly\n",
    "from evaluation.within_between_subjects import (\n",
    "    DataLoader, ExcelWriter, BaseRateAnalysis, OutcomeBiasAnalysis,\n",
    "    FalseConsensusAnalysis, SunkCostAnalysis, AllaisProblemAnalysis,\n",
    "    NonseparabilityAnalysis, FramingAnalysis, LindaProblemAnalysis,\n",
    "    AnchoringAnalysis, RelativeSavingsAnalysis, MysideBiasAnalysis,\n",
    "    OmissionBiasAnalysis, LessIsMoreAnalysis, ThalerProblemAnalysis,\n",
    "    ProbabilityMatchingAnalysis, DenominatorNeglectAnalysis\n",
    ")\n",
    "\n",
    "print(\"Running within-between subjects analysis...\")\n",
    "print(\"This analysis examines behavioral economics experiments across waves.\")\n",
    "\n",
    "# Save current directory and change to project root for the analysis\n",
    "original_cwd = Path.cwd()\n",
    "os.chdir(project_root)\n",
    "\n",
    "try:\n",
    "    # Set up data loader (now with correct relative paths from project root)\n",
    "    trial_dir_str = str(trial_dir.relative_to(project_root))\n",
    "    data_loader = DataLoader(trial_dir_str)\n",
    "    \n",
    "    # Get common IDs (respondents who completed all 4 waves)\n",
    "    try:\n",
    "        common_ids = data_loader.get_common_ids()\n",
    "        print(f\"\\n✅ Found {len(common_ids)} respondents who completed all 4 waves\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error finding common respondents: {e}\")\n",
    "        print(\"   This analysis requires the full dataset with all waves\")\n",
    "        common_ids = set()\n",
    "    \n",
    "    if len(common_ids) > 0:\n",
    "        # Load data for each wave\n",
    "        print(\"\\nLoading wave data...\")\n",
    "        try:\n",
    "            data = {\n",
    "                \"wave1\": data_loader.load_wave_data(\"wave1\", common_ids),\n",
    "                \"wave2\": data_loader.load_wave_data(\"wave2\", common_ids),\n",
    "                \"wave3\": data_loader.load_wave_data(\"wave3\", common_ids),\n",
    "                \"wave4\": data_loader.load_wave_data(\"wave4\", common_ids),\n",
    "                \"LLM\": data_loader.load_wave_data(\"LLM\", common_ids)\n",
    "            }\n",
    "            \n",
    "            for wave, df in data.items():\n",
    "                if df is not None and not df.empty:\n",
    "                    print(f\"   {wave}: {len(df)} responses loaded\")\n",
    "                else:\n",
    "                    print(f\"   {wave}: No data loaded\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error loading wave data: {e}\")\n",
    "            data = {}\n",
    "        \n",
    "        # List of all analyses\n",
    "        analyses_to_run = [\n",
    "            (\"Base Rate Neglect\", BaseRateAnalysis),\n",
    "            (\"Outcome Bias\", OutcomeBiasAnalysis),\n",
    "            (\"False Consensus Effect\", FalseConsensusAnalysis),\n",
    "            (\"Sunk Cost Fallacy\", SunkCostAnalysis),\n",
    "            (\"Allais Paradox\", AllaisProblemAnalysis),\n",
    "            (\"Non-separability of Risks\", NonseparabilityAnalysis),\n",
    "            (\"Framing Effects\", FramingAnalysis),\n",
    "            (\"Linda Problem (Conjunction Fallacy)\", LindaProblemAnalysis),\n",
    "            (\"Anchoring and Adjustment\", AnchoringAnalysis),\n",
    "            (\"Relative vs Absolute Savings\", RelativeSavingsAnalysis),\n",
    "            (\"Myside Bias\", MysideBiasAnalysis),\n",
    "            (\"Omission Bias\", OmissionBiasAnalysis),\n",
    "            (\"Less is More Effect\", LessIsMoreAnalysis),\n",
    "            (\"Thaler Problem (WTA/WTP)\", ThalerProblemAnalysis),\n",
    "            (\"Probability Matching vs Maximizing\", ProbabilityMatchingAnalysis),\n",
    "            (\"Denominator Neglect\", DenominatorNeglectAnalysis)\n",
    "        ]\n",
    "        \n",
    "        # Run a sample of analyses and display results\n",
    "        print(f\"\\nRunning {len(analyses_to_run)} behavioral economics analyses...\")\n",
    "        print(\"\\nSample Results:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # For demo, run just a few analyses and show their results\n",
    "        sample_analyses = analyses_to_run[:3]  # Run first 3 analyses for demo\n",
    "        \n",
    "        for analysis_name, analysis_class in sample_analyses:\n",
    "            print(f\"\\n📊 {analysis_name}:\")\n",
    "            try:\n",
    "                # Create a mock Excel writer that captures results\n",
    "                class MockExcelWriter:\n",
    "                    def __init__(self):\n",
    "                        self.results = []\n",
    "                    \n",
    "                    def get_unique_sheet_name(self, base_name):\n",
    "                        return base_name\n",
    "                    \n",
    "                    def write_results(self, sheet_name, results, header_note=None):\n",
    "                        self.results = results\n",
    "                        if header_note:\n",
    "                            # Fix: Extract first line without using backslash in f-string\n",
    "                            first_line = header_note.split('\\n')[0][:100]\n",
    "                            print(f\"   Description: {first_line}...\")\n",
    "                \n",
    "                mock_writer = MockExcelWriter()\n",
    "                analysis = analysis_class(mock_writer)\n",
    "                \n",
    "                # Run the analysis\n",
    "                analysis.run(data)\n",
    "                \n",
    "                # Display results\n",
    "                if mock_writer.results:\n",
    "                    for title, df in mock_writer.results[:2]:  # Show first 2 results per analysis\n",
    "                        if title and not df.empty:\n",
    "                            print(f\"\\n   {title}:\")\n",
    "                            print(df.to_string(index=False, max_rows=5))\n",
    "                            if len(df) > 5:\n",
    "                                print(f\"   ... ({len(df)-5} more rows)\")\n",
    "                else:\n",
    "                    print(\"   No results generated\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  Error: {str(e)[:100]}\")\n",
    "                if \"specific experimental questions\" in str(e) or \"KeyError\" in str(e):\n",
    "                    print(\"   This analysis requires specific questions that may not be in the limited demo\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(f\"\\n✅ Completed sample analyses. In the full pipeline, all {len(analyses_to_run)} analyses\")\n",
    "        print(\"   would be run and saved to an Excel file with detailed results.\")\n",
    "        \n",
    "        # Show where full results would be saved\n",
    "        excel_output_dir = project_root / \"text_simulation\" / \"text_simulation_output\" / \"accuracy_evaluation\"\n",
    "        output_filename = excel_output_dir / \"within_subject_analysis.xlsx\"\n",
    "        print(f\"\\n   Full results would be saved to: {output_filename}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n⚠️  No common respondents found across all waves\")\n",
    "        print(\"   This analysis requires the full dataset with respondents who completed all 4 waves\")\n",
    "        print(\"   For the demo with limited personas, this analysis cannot be performed\")\n",
    "    \n",
    "finally:\n",
    "    # Always restore the original working directory\n",
    "    os.chdir(original_cwd)\n",
    "\n",
    "print(\"\\nNote: Within-between subjects analysis examines various cognitive biases and\")\n",
    "print(\"behavioral economics phenomena across human responses and LLM simulations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Pricing Analysis\n",
    "\n",
    "Analyze pricing decisions to generate demand curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Step 11: Pricing Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import pricing analysis functions directly\n",
    "from evaluation.pricing_analysis import (\n",
    "    load_randdollar_breakdown,\n",
    "    prepare_purchase_data,\n",
    "    calculate_relative_prices\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set up paths for pricing analysis\n",
    "label_dir = trial_dir / \"csv_comparison\" / \"csv_formatted_label\"\n",
    "randdollar_file = trial_dir / \"csv_comparison\" / \"randdollar_breakdown.csv\"\n",
    "output_plot = trial_dir / \"pricing_analysis_results\" / \"average_demand_curve.png\"\n",
    "output_plot.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Running pricing demand curve analysis...\")\n",
    "\n",
    "# Check if randdollar file exists\n",
    "if not randdollar_file.exists():\n",
    "    print(f\"⚠️  Missing required file: {randdollar_file.name}\")\n",
    "    print(\"   This file is generated during JSON to CSV conversion\")\n",
    "    print(\"   Skipping pricing analysis...\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the randdollar breakdown data\n",
    "        print(\"Loading randdollar breakdown data...\")\n",
    "        df_randdollar_breakdown = load_randdollar_breakdown(str(randdollar_file))\n",
    "        \n",
    "        if df_randdollar_breakdown is None or df_randdollar_breakdown.empty:\n",
    "            print(\"⚠️  Could not load randdollar breakdown data\")\n",
    "        else:\n",
    "            print(f\"   Loaded {len(df_randdollar_breakdown)} price observations\")\n",
    "            \n",
    "            # Prepare purchase data for each wave\n",
    "            print(\"Preparing purchase data for each wave...\")\n",
    "            data_wave3 = prepare_purchase_data(df_randdollar_breakdown, \"Wave1-3\")\n",
    "            data_wave4 = prepare_purchase_data(df_randdollar_breakdown, \"Wave4\")\n",
    "            data_llm = prepare_purchase_data(df_randdollar_breakdown, \"LLM_Imputed\")\n",
    "            \n",
    "            # Combine all purchase data\n",
    "            all_purchase_data = pd.concat([data_wave3, data_wave4, data_llm], ignore_index=True)\n",
    "            \n",
    "            if all_purchase_data.empty:\n",
    "                print(\"⚠️  No purchase data could be processed\")\n",
    "            else:\n",
    "                print(f\"   Prepared {len(all_purchase_data)} purchase observations\")\n",
    "                \n",
    "                # Calculate relative prices\n",
    "                print(\"Calculating relative prices...\")\n",
    "                all_purchase_data, nprices = calculate_relative_prices(all_purchase_data)\n",
    "                \n",
    "                if nprices == 0:\n",
    "                    print(\"⚠️  Could not determine price ranks - cannot generate demand curves\")\n",
    "                else:\n",
    "                    print(f\"   Found {nprices} distinct price points\")\n",
    "                    \n",
    "                    # Compute demand curves\n",
    "                    print(\"Computing demand curves...\")\n",
    "                    demand_curves = {}\n",
    "                    for wave_name in [\"Wave1-3\", \"Wave4\", \"LLM_Imputed\"]:\n",
    "                        current_wave_data = all_purchase_data[all_purchase_data[\"Wave\"] == wave_name]\n",
    "                        if not current_wave_data.empty:\n",
    "                            curve = current_wave_data.groupby(\"Relative_Price_Rank\")[\"Purchase\"].mean()\n",
    "                            # Reindex to ensure all price ranks from 1 to nprices are present\n",
    "                            demand_curves[wave_name] = curve.reindex(range(1, nprices + 1), fill_value=np.nan)\n",
    "                        else:\n",
    "                            demand_curves[wave_name] = pd.Series([np.nan] * nprices, index=range(1, nprices + 1))\n",
    "                    \n",
    "                    # Create the plot\n",
    "                    print(\"Creating demand curve plot...\")\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    x_axis = np.arange(1, nprices + 1)\n",
    "                    \n",
    "                    # Plot each wave's demand curve\n",
    "                    if not demand_curves[\"Wave1-3\"].isna().all():\n",
    "                        plt.plot(x_axis, demand_curves[\"Wave1-3\"], linestyle='-', marker='o', label='Wave 1-3')\n",
    "                    if not demand_curves[\"Wave4\"].isna().all():\n",
    "                        plt.plot(x_axis, demand_curves[\"Wave4\"], linestyle=':', marker='s', label='Wave 4')\n",
    "                    if not demand_curves[\"LLM_Imputed\"].isna().all():\n",
    "                        plt.plot(x_axis, demand_curves[\"LLM_Imputed\"], linestyle='-.', marker='^', label='LLM Imputed (Twins)')\n",
    "                    \n",
    "                    plt.ylim(0, 1)\n",
    "                    plt.xticks(x_axis, fontsize=12)\n",
    "                    plt.yticks(fontsize=12)\n",
    "                    plt.xlabel('Relative Price Rank', fontsize=16)\n",
    "                    plt.ylabel('Purchase Probability', fontsize=16)\n",
    "                    plt.title('Average Demand Curve by Relative Price', fontsize=20)\n",
    "                    plt.legend(fontsize=12, loc='best')\n",
    "                    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                    \n",
    "                    # Save the plot\n",
    "                    plt.savefig(str(output_plot), dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    \n",
    "                    print(f\"✅ Pricing analysis completed successfully\")\n",
    "                    print(f\"   Demand curve plot saved to: {output_plot.name}\")\n",
    "                    \n",
    "                    # Try to display the plot in the notebook\n",
    "                    try:\n",
    "                        from IPython.display import Image, display\n",
    "                        display(Image(str(output_plot)))\n",
    "                    except:\n",
    "                        print(\"   (Plot saved but cannot display inline)\")\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Pricing analysis encountered an error: {e}\")\n",
    "        print(\"   This may be due to insufficient pricing data in the limited demo\")\n",
    "        print(\"   For full analysis, use the complete dataset with all personas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete digital twin simulation pipeline with evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Pipeline Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n📊 Summary of Results:\\n\")\n",
    "\n",
    "# Simulation summary\n",
    "print(\"1. SIMULATION:\")\n",
    "print(f\"   ✅ Processed {MAX_PERSONAS} personas (demo limit)\")\n",
    "print(f\"   ✅ Generated responses for survey questions\")\n",
    "\n",
    "# Check simulation outputs\n",
    "sim_output_dir = project_root / \"text_simulation\" / \"text_simulation_output\"\n",
    "if sim_output_dir.exists():\n",
    "    persona_dirs = len([d for d in sim_output_dir.iterdir() if d.is_dir() and d.name.startswith(\"pid_\")])\n",
    "    print(f\"   ✅ Created {persona_dirs} persona output directories\")\n",
    "\n",
    "# Evaluation summary\n",
    "print(\"\\n2. EVALUATION:\")\n",
    "\n",
    "# CSV conversion\n",
    "csv_dir = project_root / \"text_simulation\" / \"text_simulation_output\" / \"csv_comparison\"\n",
    "if (csv_dir / \"csv_formatted\").exists():\n",
    "    csv_count = len(list((csv_dir / \"csv_formatted\").glob(\"*.csv\")))\n",
    "    print(f\"   ✅ Generated {csv_count} CSV files for analysis\")\n",
    "\n",
    "# MAD accuracy\n",
    "accuracy_dir = project_root / \"text_simulation\" / \"text_simulation_output\" / \"accuracy_evaluation\"\n",
    "if (accuracy_dir / \"mad_accuracy_summary.xlsx\").exists():\n",
    "    print(\"   ✅ MAD accuracy evaluation completed\")\n",
    "    print(\"      - Excel summary: mad_accuracy_summary.xlsx\")\n",
    "    print(\"      - Accuracy plot: accuracy_dist.png\")\n",
    "\n",
    "# Within-between analysis\n",
    "if (accuracy_dir / \"within_subject_analysis.xlsx\").exists():\n",
    "    print(\"   ✅ Within-between subjects analysis completed\")\n",
    "\n",
    "# Pricing analysis\n",
    "pricing_dir = project_root / \"text_simulation\" / \"text_simulation_output\" / \"pricing_analysis_results\"\n",
    "if pricing_dir.exists() and any(pricing_dir.glob(\"*.png\")):\n",
    "    print(\"   ✅ Pricing demand curve analysis completed\")\n",
    "\n",
    "print(\"\\n3. KEY OUTPUTS:\")\n",
    "print(f\"   📁 All results saved to: {sim_output_dir}\")\n",
    "print(\"   📊 Key directories:\")\n",
    "print(f\"      - Simulation outputs: text_simulation_output/pid_*/\")\n",
    "print(f\"      - CSV comparisons: text_simulation_output/csv_comparison/\")\n",
    "print(f\"      - Accuracy metrics: text_simulation_output/accuracy_evaluation/\")\n",
    "print(f\"      - Pricing analysis: text_simulation_output/pricing_analysis_results/\")\n",
    "\n",
    "print(\"\\n4. NEXT STEPS:\")\n",
    "print(\"   • To process all 2058 personas, remove the MAX_PERSONAS limit\")\n",
    "print(\"   • For production runs, use the shell scripts:\")\n",
    "print(\"     - ./scripts/run_pipeline.sh (simulation)\")\n",
    "print(\"     - ./scripts/run_evaluation_pipeline.sh (evaluation)\")\n",
    "print(\"   • Review the evaluation metrics to assess digital twin quality\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ Digital Twin Simulation Pipeline Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
